# Image Recognition
## Survey
1. [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624) (2021)

## Convolution
1. [An Attention Module for Convolutional Neural Networks](https://arxiv.org/abs/2108.08205) (2021)
2. [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451) (ICCV 2021)
3. [PP-LCNet: A Lightweight CPU Convolutional Neural Network](https://arxiv.org/abs/2109.15099) (2021)
4. [Network Augmentation for Tiny Deep Learning](https://arxiv.org/abs/2110.08890) (2021)
5. [Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation](https://arxiv.org/abs/2110.11852) (NeurIPS 2021)
6. [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243) (CVPR 2021)
7. [Non-deep Networks](https://arxiv.org/abs/2110.07641) (2021)
8. [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353) (2021)
9. [On the Integration of Self-Attention and Convolution](https://arxiv.org/abs/2111.14556) (2021)
10. [NAM: Normalization-based Attention Module](https://arxiv.org/abs/2111.12419) (2021)
11. [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692) (2021)
12. [Rethinking Nearest Neighbors for Visual Classification](https://arxiv.org/abs/2112.08459) (2021)
13. [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) (2022)
14. [Omni-Dimensional Dynamic Convolution](https://openreview.net/forum?id=DmpCfq6Mg39) (ICLR 2022)
15. [You Only Cut Once: Boosting Data Augmentation with a Single Cut](https://arxiv.org/abs/2201.12078) (2022)
16. [Revisiting Dynamic Convolution via Matrix Decomposition](https://arxiv.org/abs/2103.08756) (ICLR 2021)
17. [Omnivore: A Single Model for Many Visual Modalities](https://arxiv.org/abs/2201.08377) (2022)
18. [Visual Attention Network](https://arxiv.org/abs/2202.09741) (2022)
19. [AlignMix: Improving representation by interpolating aligned features](https://arxiv.org/abs/2103.15375) (2022)
20. [Coordinate Attention for Efficient Mobile Network Design](https://arxiv.org/abs/2103.02907) (CVPR 2021)
21. [Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs](https://arxiv.org/abs/2203.06717) (CVPR 2022)
22. [On the Integration of Self-Attention and Convolution](https://arxiv.org/abs/2111.14556) (CVPR 2022)
23. [Online Convolutional Re-parameterization](https://arxiv.org/abs/2204.00826) (CVPR 2022)
24. [ResT V2: Simpler, Faster and Stronger](https://arxiv.org/abs/2204.07366) (2022)
25. [FcaNet: Frequency Channel Attention Networks](https://arxiv.org/abs/2012.11879) (ICCV 2021)
26. [PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions](https://arxiv.org/abs/2204.12511) (2022)
27. [Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs](https://arxiv.org/abs/2203.06717) (CVPR 2022)
28. [An Improved One millisecond Mobile Backbone](https://arxiv.org/abs/2206.04040) (2022)

## Transformer
1. [PSViT: Better Vision Transformer via Token Pooling and Attention Sharing](https://arxiv.org/abs/2108.03428) (2021)
2. [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302) (2021)
3. [Conformer: Local Features Coupling Global Representations for Visual Recognition](https://arxiv.org/abs/2105.03889) (2021)
4. [Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399) (2021)
5. [Mobile-Former: Bridging MobileNet and Transformer](https://arxiv.org/abs/2108.05895) (2021)
6. [Scalable Visual Transformers with Hierarchical Pooling](https://arxiv.org/abs/2103.10619) (2021)
7. [Exploring and Improving Mobile Level Vision Transformers](https://arxiv.org/abs/2108.13015) (2021)
8. [CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263) (2021)
9. [CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention](https://arxiv.org/abs/2108.00154) (2021)
10. [Global Filter Networks for Image Classification](https://arxiv.org/abs/2107.00645) (2021)
11. [Focal Self-attention for Local-Global Interactions in Vision Transformers](https://arxiv.org/abs/2107.00641) (2021)
12. [Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length](https://arxiv.org/abs/2105.15075) (2021)
13. [Bottleneck Transformers for Visual Recognition](https://arxiv.org/abs/2101.11605) (2021)
14. [Efficient Training of Visual Transformers with Small-Size Datasets](https://arxiv.org/abs/2106.03746) (2021)
15. [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899) (ICCV 2021)
16. [Fastformer: Additive Attention Can Be All You Need](https://arxiv.org/abs/2108.09084) (2021)
17. [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) (2021)
18. [Patches Are All You Need?](https://openreview.net/forum?id=TVHS5Y4dNvM) (ICLR 2022)
19. [Blending Anti-Aliasing into Vision Transformer](https://arxiv.org/abs/2110.15156) (NeurIPS 2021)
20. [Glance-and-Gaze Vision Transformer](https://arxiv.org/abs/2106.02277) (2021)
21. [Dynamic Grained Encoder for Vision Transformers](https://openreview.net/forum?id=gnAIV-EKw2) (NeurIPS 2021)
22. [All Tokens Matter: Token Labeling for Training Better Vision Transformers](https://arxiv.org/abs/2104.10858) (2021)
23. [Early Convolutions Help Transformers See Better](https://arxiv.org/abs/2106.14881) (NeurIPS 2021)
24. [TransMix: Attend to Mix for Vision Transformers](https://arxiv.org/abs/2111.09833) (2021)
25. [Self-slimmed Vision Transformer](https://arxiv.org/abs/2111.12624) (2021)
26. [PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710) (2021)
27. [Florence: A New Foundation Model for Computer Vision](https://arxiv.org/abs/2111.11432) (2021)
28. [MPViT: Multi-Path Vision Transformer for Dense Prediction](https://arxiv.org/abs/2112.11010) (2021)
29. [AdaViT: Adaptive Vision Transformers for Efficient Image Recognition](https://arxiv.org/abs/2111.15668) (2021)
30. [ATS: Adaptive Token Sampling For Efficient Vision Transformers](https://arxiv.org/abs/2111.15667) (2021)
31. [Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer](https://arxiv.org/abs/2108.01390) (2021)
32. [Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526) (2021)
33. [TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?](https://arxiv.org/abs/2106.11297) (NeurIPS 2021)
34. [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) (2021)
35. [ELSA: Enhanced Local Self-Attention for Vision Transformer](https://arxiv.org/abs/2112.12786) (2021)
36. [Lite Vision Transformer with Enhanced Self-Attention](https://arxiv.org/abs/2112.10809) (2021)
37. [SimViT: Exploring a Simple Vision Transformer with sliding windows](https://arxiv.org/abs/2112.13085) (2021)
38. [Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention](https://arxiv.org/abs/2112.14000) (2021)
39. [PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture](https://arxiv.org/abs/2201.00978) (2021)
40. [Vision Transformer with Deformable Attention](https://arxiv.org/abs/2201.00520) (2021)
41. [QuadTree Attention for Vision Transformers](https://arxiv.org/abs/2201.02767) (2022)
42. [Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903) (2022)
43. [Training Vision Transformers with Only 2040 Images](https://arxiv.org/abs/2201.10728) (2022)
44. [MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition](https://arxiv.org/abs/2201.08383) (2022)
45. [Q-ViT: Fully Differentiable Quantization for Vision Transformer](https://arxiv.org/abs/2201.07703) (2022)
46. [ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond](https://arxiv.org/abs/2202.10108) (2022)
47. [MPViT: Multi-Path Vision Transformer for Dense Prediction](https://arxiv.org/abs/2112.11010) (2022)
48. [EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers](https://arxiv.org/abs/2203.03952) (2022)
49. [ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer](https://arxiv.org/abs/2203.10790) (2022)
50. [Visualizing and Understanding Patch Interactions in Vision Transformer](https://arxiv.org/abs/2203.05922) (2022)
51. [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) (2022)
52. [DaViT: Dual Attention Vision Transformers](https://arxiv.org/abs/2204.03645) (2022)
53. [DeiT III: Revenge of the ViT](https://arxiv.org/abs/2204.07118) (2022)
54. [Shunted Self-Attention via Multi-Scale Token Aggregation](https://arxiv.org/abs/2111.15193) (CVPR 2022 Oral)
55. [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) (2022)
56. [VSA: Learning Varied-Size Window Attention in Vision Transformers](https://arxiv.org/abs/2204.08446) (2022)
57. [DeiT III: Revenge of the ViT](https://arxiv.org/abs/2204.07118) (2022)
58. [SepViT: Separable Vision Transformer](https://arxiv.org/abs/2203.15380) (2022)
59. [BOAT: Bilateral Local Attention Vision Transformer](https://arxiv.org/abs/2201.13027) (2022)
60. [EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers](https://arxiv.org/abs/2203.03952) (2022)
61. [Understanding The Robustness in Vision Transformers](https://arxiv.org/abs/2204.12451) (2022)
62. [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) (2022)
63. [Learned Queries for Efficient Local Attention](https://arxiv.org/abs/2112.11435) (CVPR 2022 Oral)
64. [Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space](https://arxiv.org/abs/2201.00814) (CVPR 2022)
65. [MiniViT: Compressing Vision Transformers with Weight Multiplexing](https://arxiv.org/abs/2204.07154) (CVPR 2022)
66. [EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers](https://arxiv.org/abs/2205.03436) (2022)
67. [TRT-ViT: TensorRT-oriented Vision Transformer](https://arxiv.org/abs/2205.09579) (2022)
68. [Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality](https://arxiv.org/abs/2205.10063) (2022)
69. [Super Vision Transformer](https://arxiv.org/abs/2205.11397) (2022)
70. [Inception Transformer](https://arxiv.org/abs/2205.12956) (2022)
71. [Green Hierarchical Vision Transformer for Masked Image Modeling](https://arxiv.org/abs/2205.13515) (2022)
72. [Fast Vision Transformers with HiLo Attention](https://arxiv.org/abs/2205.13213) (2022)
73. [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191) (2022)
74. [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) (2022)
75. [Optimizing Relevance Maps of Vision Transformers Improves Robustness](https://arxiv.org/abs/2206.01161) (2022)
76. [X-ViT: High Performance Linear Vision Transformer without Softmax](https://arxiv.org/abs/2205.13805) (2022)
77. [EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications](https://arxiv.org/abs/2206.10589) (2022)
78. [A-ViT: Adaptive Tokens for Efficient Vision Transformer](https://arxiv.org/abs/2112.07658) (CVPR 2022 Oral)
79. [Global Context Vision Transformers](https://arxiv.org/abs/2206.09959) (2022)
80. [EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications](https://arxiv.org/abs/2206.10589) (2022)

## MLP
1. [AS-MLP: An Axial Shifted MLP Architecture for Vision](https://arxiv.org/abs/2107.08391) (2021)
2. [RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?](https://arxiv.org/abs/2108.04384) (2021)
3. [Hire-MLP: Vision MLP via Hierarchical Rearrangement](https://arxiv.org/abs/2108.13341) (2021)
4. [Sparse-MLP: A Fully-MLP Architecture with Conditional Computation](https://arxiv.org/abs/2109.02008) (2021)
5. [Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?](https://arxiv.org/abs/2109.05422) (2021)
6. [ConvMLP: Hierarchical Convolutional MLPs for Vision](https://arxiv.org/abs/2109.04454) (2021)
7. [Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401) (NeuIPS 2021)
8. [An Image Patch is a Wave: Phase-Aware Vision MLP](https://arxiv.org/abs/2111.12294) (2021)
9. [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) (2021)
10. [CycleMLP: A MLP-like Architecture for Dense Prediction](https://arxiv.org/abs/2107.10224) (2022)
11. [AS-MLP: An Axial Shifted MLP Architecture for Vision](https://arxiv.org/abs/2107.08391) (2022)
12. [An Image Patch is a Wave: Quantum Inspired Vision MLP](https://arxiv.org/abs/2111.12294) (CVPR 2022)
13. [Brain-inspired Multilayer Perceptron with Spiking Neurons](https://arxiv.org/abs/2203.14679) (CVPR 2022)
14. 


## RNN
1. [Sequencer: Deep LSTM for Image Classification](https://arxiv.org/abs/2205.01972) (2022)


## AutoEncoder
1. [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) (2022)
2. [SimMIM: A Simple Framework for Masked Image Modeling](https://arxiv.org/abs/2111.09886) (2022)
3. [ConvMAE: Masked Convolution Meets Masked Autoencoders](https://arxiv.org/abs/2205.03892) (2022)

## Knowledge Distillation
1. [Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679) (CVPR 2022)
2. [Knowledge distillation: A good teacher is patient and consistent](https://arxiv.org/abs/2106.05237) (2022)

## Self-Supervised Vision Transformer
1. [An Empirical Study of Training Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.02057) (2021)
2. [Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning](https://arxiv.org/abs/2110.05340) (NeurIPS 2021)

## Neural Architecture Search
1. [Searching for Efficient Multi-Stage Vision Transformers](https://arxiv.org/abs/2109.00642v1) (2021)

## Others
1. [Tune It or Don’t Use It: Benchmarking Data-Efficient Image Classification](https://arxiv.org/abs/2108.13122) (ICCV 2021)
2. [A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP](https://arxiv.org/abs/2108.13002) (2021)
3. [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359) (ICCV 2021)
4. 

## Visual Feature Attribution
1. [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861) (ICCV 2021)
2. 

## Fine-grained Visual Recognition
1. [A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition](https://arxiv.org/abs/2110.01240) (2021)
2. [Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information](https://arxiv.org/abs/2203.03253) (CVPR 2022)

## Self-supervised
1. [SiT: Self-supervised vIsion Transformer](https://arxiv.org/abs/2104.03602) (2021)
2. [Crafting Better Contrastive Views for Siamese Representation Learning](https://arxiv.org/abs/2202.03278) (CVPR 2022)
3. [Self-Supervised Visual Representation Learning with Semantic Grouping](https://proceedings.neurips.cc/paper/2020/hash/c1502ae5a4d514baec129f72948c266e-Abstract.html) (NeurIPS 2020)

## Small Datasets
1. [Efficient Training of Visual Transformers with Small Datasets](https://arxiv.org/abs/2106.03746) (2021)


