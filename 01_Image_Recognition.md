# Image Recognition
## Survey
1. [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624) (2021)

## Convolution
1. [An Attention Module for Convolutional Neural Networks](https://arxiv.org/abs/2108.08205) (2021)
2. [Unifying Nonlocal Blocks for Neural Networks](https://arxiv.org/abs/2108.02451) (ICCV 2021)
3. [PP-LCNet: A Lightweight CPU Convolutional Neural Network](https://arxiv.org/abs/2109.15099) (2021)
4. [Network Augmentation for Tiny Deep Learning](https://arxiv.org/abs/2110.08890) (2021)
5. [Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation](https://arxiv.org/abs/2110.11852) (NeurIPS 2021)
6. [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243) (CVPR 2021)
7. [Non-deep Networks](https://arxiv.org/abs/2110.07641) (2021)
8. [Can Vision Transformers Perform Convolution?](https://arxiv.org/abs/2111.01353) (2021)
9. 

## Transformer
1. [PSViT: Better Vision Transformer via Token Pooling and Attention Sharing](https://arxiv.org/abs/2108.03428) (2021)
2. [Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302) (2021)
3. [Conformer: Local Features Coupling Global Representations for Visual Recognition](https://arxiv.org/abs/2105.03889) (2021)
4. [Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399) (2021)
5. [Mobile-Former: Bridging MobileNet and Transformer](https://arxiv.org/abs/2108.05895) (2021)
6. [Scalable Visual Transformers with Hierarchical Pooling](https://arxiv.org/abs/2103.10619) (2021)
7. [Exploring and Improving Mobile Level Vision Transformers](https://arxiv.org/abs/2108.13015) (2021)
8. [CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263) (2021)
9. [CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention](https://arxiv.org/abs/2108.00154) (2021)
10. [Global Filter Networks for Image Classification](https://arxiv.org/abs/2107.00645) (2021)
11. [Focal Self-attention for Local-Global Interactions in Vision Transformers](https://arxiv.org/abs/2107.00641) (2021)
12. [Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length](https://arxiv.org/abs/2105.15075) (2021)
13. [Bottleneck Transformers for Visual Recognition](https://arxiv.org/abs/2101.11605) (2021)
14. [Efficient Training of Visual Transformers with Small-Size Datasets](https://arxiv.org/abs/2106.03746) (2021)
15. [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899) (ICCV 2021)
16. [Fastformer: Additive Attention Can Be All You Need](https://arxiv.org/abs/2108.09084) (2021)
17. [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) (2021)
18. [Patches Are All You Need?](https://openreview.net/forum?id=TVHS5Y4dNvM) (ICLR 2022)
19. [Blending Anti-Aliasing into Vision Transformer](https://arxiv.org/abs/2110.15156) (NeurIPS 2021)
20. [Glance-and-Gaze Vision Transformer](https://arxiv.org/abs/2106.02277) (2021)
21. [Dynamic Grained Encoder for Vision Transformers](https://openreview.net/forum?id=gnAIV-EKw2) (NeurIPS 2021)
22. [All Tokens Matter: Token Labeling for Training Better Vision Transformers](https://arxiv.org/abs/2104.10858) (2021)
23. [Early Convolutions Help Transformers See Better](https://arxiv.org/abs/2106.14881) (NeurIPS 2021)
24. [TransMix: Attend to Mix for Vision Transformers](https://arxiv.org/abs/2111.09833) (2021)
25. [Self-slimmed Vision Transformer](https://arxiv.org/abs/2111.12624) (2021)
26. [PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710) (2021)
27. [Florence: A New Foundation Model for Computer Vision](https://arxiv.org/abs/2111.11432) (2021)
28. [MPViT: Multi-Path Vision Transformer for Dense Prediction](https://arxiv.org/abs/2112.11010) (2021)

## MLP
1. [AS-MLP: An Axial Shifted MLP Architecture for Vision](https://arxiv.org/abs/2107.08391) (2021)
2. [RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?](https://arxiv.org/abs/2108.04384) (2021)
3. [Hire-MLP: Vision MLP via Hierarchical Rearrangement](https://arxiv.org/abs/2108.13341) (2021)
4. [Sparse-MLP: A Fully-MLP Architecture with Conditional Computation](https://arxiv.org/abs/2109.02008) (2021)
5. [Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?](https://arxiv.org/abs/2109.05422) (2021)
6. [ConvMLP: Hierarchical Convolutional MLPs for Vision](https://arxiv.org/abs/2109.04454) (2021)
7. [Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401) (NeuIPS 2021)
8. [An Image Patch is a Wave: Phase-Aware Vision MLP](https://arxiv.org/abs/2111.12294) (2021)
9. [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) (2021)

## Self-Supervised Vision Transformer
1. [An Empirical Study of Training Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.02057) (2021)
2. [Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning](https://arxiv.org/abs/2110.05340) (NeurIPS 2021)

## Neural Architecture Search
1. [Searching for Efficient Multi-Stage Vision Transformers](https://arxiv.org/abs/2109.00642v1) (2021)

## Others
1. [Tune It or Donâ€™t Use It: Benchmarking Data-Efficient Image Classification](https://arxiv.org/abs/2108.13122) (ICCV 2021)
2. [A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP](https://arxiv.org/abs/2108.13002) (2021)
3. [Towards Learning Spatially Discriminative Feature Representations](https://arxiv.org/abs/2109.01359) (ICCV 2021)
4. 

## Visual Feature Attribution
1. [Keep CALM and Improve Visual Feature Attribution](https://arxiv.org/abs/2106.07861) (ICCV 2021)
2. 

## Fine-grained Visual Recognition
1. [A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition](https://arxiv.org/abs/2110.01240) (2021)

## Self-supervised
1. [SiT: Self-supervised vIsion Transformer](https://arxiv.org/abs/2104.03602) (2021)




